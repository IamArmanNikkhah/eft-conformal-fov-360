# ğŸ¥ PEFT-Conformal FoV Prediction for 360Â° Video Streaming

> University of Texas at Dallas â€” Immersive Media Systems Lab  

---

## ğŸ§  Overview

This project explores how **personalized multimodal Transformers** and **conformal prediction (CP)** can improve **360Â° video streaming** under unstable networks.

You will build a small simulator that:
1. Predicts a userâ€™s **future field of view (FoV)** using a lightweight Transformer.
2. Personalizes that model per user using **PEFT (LoRA/adapter)** fine-tuning.
3. Produces **set-valued FoV regions** via **split conformal prediction**, ensuring a bounded miss-rate (â‰¤ Î±).
4. Adapts Î± online through a feedback controller to trade off **risk vs. bandwidth**.

Your system will be evaluated by **Quality-of-Experience (QoE)** metrics such as viewport hit ratio, rebuffer ratio, and VWS-PSNR/VMAF-360 quality.

---

## ğŸ§© Repository Structure

```

peft-conformal-fov/
â”œâ”€â”€ src/                # all source code
â”‚   â”œâ”€â”€ geometry.py     # yaw/pitch â†” vector, geodesic distance
â”‚   â”œâ”€â”€ dataset.py      # data loaders (AVTrack360, Deep360Pilot)
â”‚   â”œâ”€â”€ erp_grid.py     # ERP tiling utilities (6x12 default)
â”‚   â”œâ”€â”€ player_stub.py  # simulation skeleton
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tests/              # pytest unit tests
â”œâ”€â”€ scripts/            # runnable scripts (e.g., prep_data.py, run_sim.py)
â”œâ”€â”€ configs/            # YAML/JSON config files
â”œâ”€â”€ data/               # local datasets (ignored by Git)
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ notebooks/          # Jupyter notebooks for quick experiments
â”œâ”€â”€ env.yml             # Conda environment
â”œâ”€â”€ requirements.txt    # pip alternative (optional)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE

````

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/<your-username>/peft-conformal-fov.git
cd peft-conformal-fov
````

### 2. Create and Activate Environment (Conda)

```bash
conda env create -f env.yml
conda activate fovenv
```

or, using pip:

```bash
python -m venv venv
source venv/bin/activate        # (Windows: venv\Scripts\activate)
pip install -r requirements.txt
```

### 3. Verify Installation

```bash
pytest
```

You should see:

```
collected 1 item
tests/test_basic.py .                                         [100%]
```

---

## ğŸ“¦ Datasets

Weâ€™ll use publicly available 360Â° head-motion datasets.
Download them manually (theyâ€™re large â€” donâ€™t push to GitHub!):

| Dataset          | Description                                           | Link                                                                 |
| ---------------- | ----------------------------------------------------- | -------------------------------------------------------------------- |
| **AVTrack360**   | Real head movements of users watching 360Â° videos     | [AVTrack360 dataset](https://github.com/AndreyTrekhleb/AVTrack360)   |
| **Deep360Pilot** | Trajectories for â€œpilotingâ€ tasks in immersive videos | [Deep360Pilot dataset](https://github.com/deep360pilot/deep360pilot) |

After downloading, place the raw files in:

```
data/AVTrack360/
data/Deep360Pilot/
```

and update their paths in your local `.env` or config file.

---

## ğŸš€ Quick Start (After Setup)

1. **Run the player stub** to inspect data flow:

   ```bash
   python -m src.player_stub --user 1 --seconds 30
   ```

   â†’ outputs a CSV log of head orientation and tile indices.

2. **Train pooled Transformer (Week 2):**

   ```bash
   python scripts/train_pooled.py
   ```

3. **Fine-tune per-user adapters (Week 3):**

   ```bash
   python scripts/tune_peft.py --user 5
   ```

4. **Calibrate Conformal Prediction (Week 4):**

   ```bash
   python scripts/calibrate_cp.py
   ```

5. **Run full simulation with Î±-controller (Week 5):**

   ```bash
   python scripts/run_sim.py --config configs/default.yaml
   ```

---

## ğŸ§ª Testing

Run all unit tests anytime:

```bash
pytest -v
```

Each new module should include small, isolated tests in `tests/`.

For code style checks (if pre-commit installed):

```bash
pre-commit run --all-files
```

---

## ğŸ“Š Evaluation Metrics

During final evaluation (Week 6), youâ€™ll log:

* **Viewport Hit Ratio** â€“ tiles within viewport correctly prefetched.
* **VWS-PSNR / VMAF-360** â€“ perceptual quality inside the viewport.
* **Rebuffer Ratio** â€“ stall time Ã· total play time.
* **Miss-Rate Î±Ì‚** â€“ fraction of frames where actual FoV lies outside CP set.

All results are aggregated across users and bandwidth conditions.

---

## ğŸ‘©â€ğŸ’» Team Roles (Recommended)

| Role                     | Focus                                            |
| ------------------------ | ------------------------------------------------ |
| **Lead / Integrator**    | Repository hygiene, CI, experiment orchestration |
| **Model Lead**           | Transformer + PEFT tuning                        |
| **Personalization Lead** | Adapter integration, per-user pipelines          |
| **Calibration Lead**     | Conformal prediction and Î±-control               |
| **Systems Lead**         | Tiling, QoE metrics, and network emulation       |

Rotate tasks weekly so everyone touches both ML and systems code.

---

## ğŸ—“ï¸ Weekly Milestones

| Week | Focus                               | Key Deliverable              |
| ---- | ----------------------------------- | ---------------------------- |
| 1    | Environment, repo, data, geometry   | Working loader + player stub |
| 2    | Base Transformer + baselines        | Trained pooled model         |
| 3    | PEFT personalization                | Per-user adapters            |
| 4    | Conformal sets + tile mapping       | Validated coverage control   |
| 5    | Online Î±-controller + streaming sim | QoE comparisons              |
| 6    | Evaluation + presentation           | Final report + demo          |

---

## ğŸ§° Tech Stack

* **Language:** Python 3.11
* **Core Libraries:** PyTorch, NumPy, pandas, SciPy, matplotlib, tqdm, PEFT
* **Version Control:** Git + GitHub
* **Testing:** pytest
* **Optional:** W&B or MLflow for experiment tracking
* **Video Metrics:** VMAF-360 or VWS-PSNR tools

---

## ğŸ’¡ Tips for Success

* Run small tests often â€” each module should work in isolation.
* Donâ€™t push dataset files; use `.gitignore` wisely.
* Document parameters and choices in `configs/` for reproducibility.
* Keep your commits small and descriptive (e.g., â€œAdd geodesic distance functionâ€).
* Sync with your team daily â€” this project builds like Lego; each block depends on the last.


---

## ğŸ“œ License

MIT License â€” free to use and modify with attribution.

---

## ğŸ“¨ Contact

Questions or issues?
Open an [issue](https://github.com/<your-username>/peft-conformal-fov/issues) or contact your course mentor.

---

*Happy coding, and remember: start simple, test everything, and commit early!* âœ¨

```

---

### ğŸ’¬ Mentorâ€™s Note
This README does three things at once:
1. **Educates:** briefly explains the purpose and context.  
2. **Guides:** provides clear setup and usage steps.  
3. **Aligns:** shows milestones and responsibilities.

Once you commit this to `README.md` on GitHub, your repo will instantly look professional and be usable by any new team member or reviewer.
```
